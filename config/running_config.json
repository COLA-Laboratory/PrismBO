{
    "tasks": [
        {
            "name": "HPO_PINN",
            "description": "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, which must be configured before the process starts. Physics-informed neural networks (PINNs), also referred to as Theory-Trained Neural Networks (TTNs), are a type of universal function approximators that can embed the knowledge of any physical laws that govern a given data-set in the learning process, and can be described by partial differential equations (PDEs). Low data availability for some biological and engineering problems limit the robustness of conventional machine learning models used for these applications. The prior knowledge of general physical laws acts in the training of neural networks (NNs) as a regularization agent that limits the space of admissible solutions, increasing the generalizability of the function approximation. This way, embedding this prior information into a neural network results in enhancing the information content of the available data, facilitating the learning algorithm to capture the right solution and to generalize well even with a low amount of training examples. For they process continuous spatial and time coordinates and output continuous PDE solutions, they can be categorized as neural fields.",
            "num_vars": "5",
            "num_objs": "1",
            "workloads": "0,1,2",
            "budget_type": "function evaluations",
            "budget": "55"
        },
        {
            "name": "HPO_ResNet18",
            "description": "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, which must be configured before the process starts. A residual neural network (also referred to as a residual network or ResNet) is a deep learning architecture in which the layers learn residual functions with reference to the layer inputs. It was developed in 2015 for image recognition, and won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) of that year.",
            "num_vars": "5",
            "num_objs": "1",
            "workloads": "0,1,2",
            "budget_type": "function evaluations",
            "budget": "55"

        }
    ],
    
    "optimizer": {
        "SearchSpace": {
            "type": "None",
            "auxiliaryData": [],
            "autoSelect": false
        },
        "Initialization": {
            "type": "random",
            "InitNum": 5,
            "auxiliaryData": [],
            "autoSelect": false
        },
        "AcquisitionFunction": {
            "type": "EI",
            "auxiliaryData": [],
            "autoSelect": false
        },
        "Pretrain": {
            "type": "None",
            "auxiliaryData": [],
            "autoSelect": false
        },
        "Model": {
            "type": "RGPE",
            "auxiliaryData": [],
            "autoSelect": true
        },
        "Normalizer": {
            "type": "Standard",
            "auxiliaryData": [],
            "autoSelect": false
        }
    },
    "seeds": "0",
    "remote": false,
    "server_url": "",
    "experimentName": "Open-ended"
}